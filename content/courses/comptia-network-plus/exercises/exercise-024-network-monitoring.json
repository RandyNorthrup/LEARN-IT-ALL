{
  "id": "exercise-024-network-monitoring",
  "lessonId": "lesson-024-network-monitoring",
  "title": "Comprehensive Network Monitoring Implementation and Troubleshooting",
  "description": "You are implementing a comprehensive network monitoring solution for MedTech Industries. Currently, the IT team has no visibility into network performance, and users frequently report issues but provide vague descriptions. Your task is to configure SNMPv3 monitoring on all devices, implement NetFlow for traffic analysis, establish centralized syslog, and use collected data to troubleshoot real performance issues. You must design monitoring to detect both normal operations and critical alerts while minimizing false positives.",
  "difficulty": "advanced",
  "points": 150,
  "language": "cisco-ios",
  "scenario": {
    "company": "MedTech Industries",
    "situation": "No centralized network monitoring. Users report 'network is slow' with no IT visibility. Help desk cannot troubleshoot effectively. No alerting system exists.",
    "infrastructure": {
      "core_devices": [
        "Core-Router-01 (10.1.1.1)",
        "Core-Router-02 (10.1.1.2)",
        "Core-SW-01 (10.1.1.10)",
        "Dist-SW-01 (10.2.1.1)",
        "Dist-SW-02 (10.2.1.2)"
      ],
      "monitoring_server": "10.1.1.200 (Grafana/LibreNMS)",
      "syslog_server": "10.1.1.201",
      "ntp_servers": ["10.1.1.50 (primary)", "10.1.1.51 (backup)"],
      "critical_interfaces": [
        "GigabitEthernet0/1 (Internet Uplink - 100 Mbps)",
        "GigabitEthernet0/2 (Core backbone - 1 Gbps)",
        "GigabitEthernet0/3 (Distribution uplink - 1 Gbps)"
      ],
      "users": "500 employees across 3 buildings",
      "applications": "EHR system (medical records), financial applications, file services, VoIP phones"
    },
    "business_requirements": [
      "Monitor CPU and memory on all devices (alert if >80% for 5 minutes)",
      "Track interface utilization (alert if >75% on critical links)",
      "Centralize all device logs for compliance and troubleshooting",
      "Detect interface errors and downs immediately",
      "Provide data for capacity planning (which links need upgrades)",
      "Support root cause analysis of 'network is slow' complaints"
    ],
    "current_problems": [
      "Users report slow EHR application, but no baseline to compare against",
      "Internet connection appears congested during 11am-1pm and 3pm-5pm",
      "No way to know if problem is network or application server",
      "Two network devices had silent failures (discovered only when manual reboot)",
      "Cannot correlate events across multiple devices to find root cause",
      "No historical data for trend analysis or capacity planning"
    ]
  },
  "starterCode": "# COMPREHENSIVE NETWORK MONITORING IMPLEMENTATION\n# MedTech Industries - Monitoring Solution Design\n# You must configure monitoring on all core devices and troubleshoot using collected data\n\n# ===================================================================\n# SCENARIO 1: SNMPv3 SECURE DEVICE MONITORING\n# ===================================================================\n# PROBLEM: No centralized visibility into device health (CPU, memory, interfaces)\n# TASK: Configure SNMPv3 on Core-Router-01 to allow secure monitoring\n\n# Challenge 1.1: Configure SNMPv3 with proper security levels\n# SNMPv3 must include:\n# - Authentication (SHA-256 for integrity)\n# - Encryption (AES-128 for confidentiality)\n# - Access control (read-only community for monitoring)\n#\n# Questions:\n# Q1: Why use SNMPv3 instead of SNMPv2c (which is simpler)?\n# Q2: What are the three components of SNMPv3 security model?\n# Q3: Why is 'priv' security level necessary for monitoring medical infrastructure?\n#\n# Challenge 1.2: Configure SNMP traps for critical alerts\n# Your monitoring system needs immediate notification of:\n# - Interface status changes (down/up events)\n# - Configuration changes (someone modifying device)\n# - CPU threshold exceeded (>80% for 5+ minutes)\n# - Memory low condition\n#\n# Questions:\n# Q4: What SNMP trap destinations should receive alerts?\n# Q5: How would you avoid alert fatigue from too many traps?\n# Q6: Design an escalation: minor alerts to Grafana, critical alerts to SMS/PagerDuty\n#\n# Challenge 1.3: Configure MIBs and OIDs for useful metrics\n# Standard MIBs to monitor:\n# - sysUpTime (system uptime)\n# - sysDescr (device description)\n# - ifOperStatus (interface up/down state)\n# - ifInOctets / ifOutOctets (traffic rates)\n# - cpuUsage (CPU percentage)\n# - memoryUsage (RAM available)\n#\n# Questions:\n# Q7: Which OID would you query to detect if Core-Router-01 rebooted?\n# Q8: How would you calculate interface bandwidth utilization from OID values?\n# Q9: Set up collection to sample every 5 minutes (detect short spikes? or use 30min?)\n\n# Configuration template for Core-Router-01 SNMPv3:\n# snmp-server group NETOPS v3 priv read VIEWS_SYSTEM write VIEWS_ALL\n# snmp-server user snmp_monitor NETOPS v3 auth sha <AUTH_PASS> priv aes 128 <PRIV_PASS>\n# snmp-server view VIEWS_SYSTEM included 1.3.6.1.2.1\n# snmp-server host 10.1.1.200 version 3 priv snmp_monitor\n# snmp-server enable traps snmp linkdown linkup\n# snmp-server enable traps cpu threshold\n\n# ===================================================================\n# SCENARIO 2: NETFLOW FOR TRAFFIC ANALYSIS AND TROUBLESHOOTING\n# ===================================================================\n# PROBLEM: \"Network is slow\" complaint, but no data on what's consuming bandwidth\n# TASK: Configure NetFlow to see actual traffic patterns\n\n# Challenge 2.1: Design NetFlow collector architecture\n# Question: Why collect NetFlow on routers, not switches?\n# Question: At what layer does NetFlow operate (provide details)?\n# Question: What information does NetFlow provide that SNMP cannot?\n#\n# Challenge 2.2: Configure NetFlow export and collection\n# NetFlow exports to: 10.1.1.200 (LibreNMS/Grafana)\n# Export protocol: NetFlow v5 (compatible with older tools) or v9 (flexible)?\n#\n# Your decision and justification:\n# - If v5: Why (compatibility, simple)?\n# - If v9: Why (IPFIX protocol, flexible)?\n# - How does template-based v9 differ from fixed v5?\n#\n# Questions:\n# Q10: Configure NetFlow on Internet uplink (Gi0/1) - is input, output, or both needed?\n# Q11: Why would you sample NetFlow (1:100) on high-speed interfaces?\n# Q12: What UDP port should NetFlow use (2055, 9996, 5678)? Justify choice.\n#\n# Challenge 2.3: Use NetFlow data to answer \"network is slow\"\n# Scenario: Users report slow EHR access between 11am-1pm\n# NetFlow data shows:\n# - 10.2.0.0/16 (client workstations) → 10.3.10.50 (EHR server): 450 Mbps\n# - 10.2.0.0/16 → 8.8.8.8 (YouTube): 320 Mbps\n# - 10.2.0.0/16 → 1.1.1.1 (DNS): 45 Mbps\n# - Internet uplink total: 100 Mbps (ISP contract max)\n#\n# Q13: What's the problem and root cause?\n# Q14: Who is actually causing the EHR slowdown?\n# Q15: What is the immediate remediation?\n# Q16: Long-term solution?\n\n# Configuration template for NetFlow:\n# flow exporter NETFLOW_EXPORT\n#   destination 10.1.1.200\n#   transport udp 2055\n#   export-protocol netflow-v9\n#   option interface-table\n#   option sampler-table\n# !\n# flow monitor NETFLOW_MONITOR\n#   exporter NETFLOW_EXPORT\n#   record netflow-original\n#   timeout active 60\n#   timeout inactive 30\n# !\n# interface GigabitEthernet0/1\n#   ip flow monitor NETFLOW_MONITOR input\n#   ip flow monitor NETFLOW_MONITOR output\n\n# ===================================================================\n# SCENARIO 3: CENTRALIZED SYSLOG FOR EVENT CORRELATION\n# ===================================================================\n# PROBLEM: Network device silently fails, discovered only hours later\n# TASK: Configure centralized syslog to capture all events\n\n# Challenge 3.1: Syslog severity levels\n# Define which events each device should log:\n# - Emergency/Alert: Device crashes, interface card failures\n# - Critical: Configuration authentication failures, memory errors\n# - Error: Interface errors, OSPF adjacency lost\n# - Warning: High CPU, high memory (>80%)\n# - Informational: Configuration saves, system startups\n# - Debug: Very verbose (usually only when troubleshooting)\n#\n# Q17: What severity level should be default (capture everything or just important)?\n# Q18: Why would logging too much create problems (hint: CPU, storage, bandwidth)?\n# Q19: Design a logging filter: all devices log Errors+Warnings, core devices add Info\n#\n# Challenge 3.2: Accurate timestamps for correlation\n# Problem: Logs from 5 devices arrive at syslog server, timestamps differ by 1+ hours\n# Root cause: Devices use local clocks (no NTP) and haven't been synchronized\n#\n# Q20: How would you configure NTP to solve this?\n# Q21: Why is NTP critical for network troubleshooting?\n# Q22: Design NTP hierarchy: which devices are primary, secondary, clients?\n#\n# Challenge 3.3: Parse syslog to detect issues\n# Example log entries:\n# Nov 30 10:15:22 Core-Router-01 %CPU: CPU util = 92%, Free memory = 4%, Alert!\n# Nov 30 10:15:25 Core-Router-01 %INTERFACE: GigabitEthernet0/2 down\n# Nov 30 10:15:30 Core-Router-01 %OSPF: Lost neighbor 10.1.1.2\n# Nov 30 10:15:35 Core-Router-01 %BGP: Route flap (unstable route)\n#\n# Q23: Parse this log sequence. What's the likely problem?\n# Q24: What events would you trigger alerts on (create alarm)?\n# Q25: Which events are symptoms vs root cause?\n#\n# Configuration template for Syslog:\n# logging host 10.1.1.201\n# logging trap informational\n# service timestamps log datetime msec localtime show-timezone\n# service sequence-numbers\n# logging source-interface Loopback0\n\n# ===================================================================\n# SCENARIO 4: MONITORING DATA TO TROUBLESHOOT REAL PROBLEMS\n# ===================================================================\n# PROBLEM: Use monitoring to answer actual IT questions\n# TASK: Analyze collected data to make decisions\n\n# Challenge 4.1: Capacity Planning\n# Historical traffic data (30 days, Internet link Gi0/1):\n# - Average: 45 Mbps\n# - 95th percentile: 78 Mbps  \n# - Peak (5-minute average): 92 Mbps\n# - ISP contract: 100 Mbps\n# - Trend: Growing 3% per month\n#\n# Q26: When should you upgrade the ISP connection (recommendation: >70% sustained)?\n# Q27: What capacity should you upgrade to (options: 200Mbps, 300Mbps, 500Mbps)?\n# Q28: Budget planning: If upgrade costs $2,000/month, justify to CFO (ROI, risk)\n#\n# Challenge 4.2: Interface error analysis\n# Interface statistics for Core-SW-01 Gi0/1 (trunk to Dist-SW-01):\n# - Input errors: 247 in 30 days\n# - Output errors: 0\n# - CRC errors: 198\n# - Frames too long (Jumbo frames issue): 49\n# - Utilization: 12% average, 35% peak\n#\n# Q29: Are these errors significant? Should you worry?\n# Q30: What is the likely cause of CRC errors?\n# Q31: Root cause investigation: What would you check? (cable, NIC, speed/duplex mismatch)\n# Q32: Is this a \"fix immediately\" or \"monitor and plan upgrade\" situation?\n#\n# Challenge 4.3: CPU spike investigation\n# CPU graph shows:\n# - Baseline: 15-25%\n# - Daily spike 11am-1pm: 65-75%\n# - Weekend: 8-12%\n# - Related to: Finance reports being generated (happens at 11am daily)\n#\n# Q33: Is this CPU spike a problem (yes/no)? Why?\n# Q34: If yes, what's the solution? (more capacity, disable features, optimize)\n# Q35: What metric would indicate actual problems (not just normal load)?\n#\n# Challenge 4.4: Correlation analysis\n# Timeline of events (Nov 30):\n# 10:30 - EHR users report \"slow performance\"\n# 10:32 - NetFlow shows 450 Mbps to 8.8.8.8 (DNS query traffic anomaly)\n# 10:35 - Syslog shows DNS query timeout, then retry storm\n# 10:38 - Syslog shows Dist-SW-01 CPU spike to 92%\n# 10:40 - Interface Gi0/1 error count increases rapidly\n# 10:45 - Suddenly returns to normal\n#\n# Q36: What's the sequence of events (root cause → symptom → symptom)?\n# Q37: What was the actual root cause?\n# Q38: Design a monitoring rule to detect this BEFORE users notice\n#\n# ===================================================================\n# DELIVERABLE CHECKLIST\n# ===================================================================\n# Your implementation must include:\n# [ ] SNMPv3 configuration (security, traps, MIBs)\n# [ ] NetFlow configuration (exporter, sampling, analysis)\n# [ ] Syslog configuration (centralization, timestamps, filtering)\n# [ ] NTP configuration (time synchronization)\n# [ ] Answers to all 38 scenario questions\n# [ ] Design document for monitoring architecture\n# [ ] Alert configuration and thresholds\n# [ ] Capacity planning recommendations (3-year projection)\n# [ ] Troubleshooting guide using monitoring data\n# [ ] ROI calculation for monitoring implementation\n\n# Each answer must be:\n# - Technically justified\n# - Specific (not vague)\n# - Professional-quality documentation\n# - Ready to present to CIO/management",
  "solution": "# COMPREHENSIVE NETWORK MONITORING IMPLEMENTATION - SOLUTION\n# MedTech Industries - Complete Monitoring Solution\n\n## ===================================================================\n## SCENARIO 1: SNMPv3 SECURE DEVICE MONITORING (SOLUTION)\n## ===================================================================\n\n### Challenge 1.1: SNMPv3 Configuration Justification\n\n**Q1: Why use SNMPv3 instead of SNMPv2c?**\nAnswer: SNMPv2c uses community strings transmitted in clear text. Anyone on the network can read \"public\" community and change device configuration. Medical facilities have HIPAA compliance requirements. SNMPv3 provides:\n- Authentication (verify data integrity, prevents modification)\n- Encryption (prevents eavesdropping on sensitive metrics like CPU, memory, interface configs)\n- Per-user access control (different permissions for different roles)\n- Traceability (logs show which user made changes)\n\n**Q2: Three components of SNMPv3 security model:**\n1. **User/Authentication** (who is accessing) - Username, authentication algorithm (SHA/MD5), authentication password\n2. **Privacy/Encryption** (what they see) - Encryption algorithm (AES/DES), encryption key\n3. **Security Level** (what protection applied):\n   - noAuthNoPriv: No authentication, no encryption (rarely used)\n   - authNoPriv: Authentication only (faster, some protection)\n   - authPriv: Both authentication and encryption (full security)\n\n**Q3: Why \"priv\" level necessary for medical infrastructure?**\nAnswer: \n- HIPAA Compliance: Patient data might flow through network interfaces being monitored\n- Competitive Advantage: Interface traffic patterns reveal business operations\n- Regulatory: Healthcare facilities audited for compliance\n- Risk: Competitor could see network design, capacity, security gaps\nConfiguration must be: `snmp-server group MEDTECH v3 priv`\n\n### Challenge 1.2: SNMP Traps Configuration\n\n**Q4: SNMP trap destinations**\n- Primary: 10.1.1.200 (LibreNMS/Grafana dashboard)\n- Backup: 10.1.1.201 (Secondary monitoring server)\n- Both receive all alerts redundantly (if primary fails, alerts still received)\n\n**Q5: Avoid alert fatigue - severity filtering**\n- Configure different severity levels per device type\n- Core devices: traps on errors, warnings, critical\n- Access switches: traps on critical, interface down only\n- Use trap throttling (if same trap repeats >10/minute, suppress duplicates)\n- Create trap correlation rules (if multiple interfaces fail on same switch, fire one \"switch failure\" alert, not 48 interface alerts)\n\n**Q6: Escalation design**\n- Minor alerts (interface errors <100/day): Dashboard notification only\n- Warning alerts (interface down, CPU >85%, memory >90%): Email to NetOps team\n- Critical alerts (device down, multiple interface failures, config changes): SMS + PagerDuty + automatic failover if available\n- Commands:\n  ```\n  snmp-server host 10.1.1.200 version 3 priv snmp_monitor\n  snmp-server enable traps snmp linkdown linkup\n  snmp-server enable traps config\n  snmp-server enable traps cpu threshold\n  snmp-server enable traps memory\n  ```\n\n### Challenge 1.3: MIBs and OIDs\n\n**Q7: Which OID detects reboot?**\nAnswer: sysUpTime (1.3.6.1.2.1.1.3.0)\n- If sysUpTime decreases from last query, device rebooted\n- Example: Last query 10:00am showed sysUpTime=2,592,000 seconds (30 days), next query 10:05am shows 300 seconds (5 minutes) = device rebooted\n\n**Q8: Calculate interface utilization from OIDs**\nFormula: (ifInOctets_now - ifInOctets_previous) × 8 / (sample_time × 1,000,000) = Mbps\nExample: \n- ifInOctets at 10:00am = 1,000,000,000 bytes\n- ifInOctets at 10:05am = 1,037,500,000 bytes\n- Bytes/5min = (1,037,500,000 - 1,000,000,000) = 37,500,000 bytes\n- Bits/5min = 37,500,000 × 8 = 300,000,000 bits\n- Bps = 300,000,000 / 300 seconds = 1,000,000 bps\n- Mbps = 1,000,000 / 1,000,000 = 1 Mbps\n\n**Q9: Sampling interval decision**\nAnswer: 5-minute intervals optimal because:\n- Captures traffic patterns (business hours spike visible)\n- Detects problems before SLA violation (most SLAs measure 5-minute averages)\n- Stores reasonable data volume (288 samples/day × 30 days × 5 devices = 43,200 data points)\n- Responds fast to issues (5-min lag acceptable for network operations)\n- NOT 30min: Would miss spike incidents that resolve in 15-20 minutes\n\n## ===================================================================\n## SCENARIO 2: NETFLOW FOR TRAFFIC ANALYSIS (SOLUTION)\n## ===================================================================\n\n**Q10: NetFlow on Internet uplink - input, output, or both?**\nAnswer: BOTH required\n- Input: Inbound traffic from Internet (monitoring ISP usage, incoming attacks)\n- Output: Outbound traffic (which internal users consuming bandwidth)\n- Single direction would miss half the picture (YouTube downloads are output from user perspective, input from router perspective)\n\n**Q11: Why sample NetFlow on high-speed interfaces?**\nAnswer: Processing overhead\n- 100 Mbps interface generating 10,000 flows/second\n- Processing every flow: CPU spike, memory exhaustion\n- Sampling 1:100 means record 1 of every 100 flows\n- Extrapolate results: If sample shows 1 Mbps traffic, actual is ~100 Mbps\n- Trade-off: Lose granularity (don't see individual small flows), but maintain stability\n- On 1 Gbps link: Sampling 1:1000 required for stability\n\n**Q12: UDP port for NetFlow**\nAnswer: 2055 (industry standard for NetFlow v5)\n- NetFlow v5: 2055 (traditional, widely supported)\n- IPFIX (NetFlow v9+): 4739 (newer standard, more flexible)\n- Custom: 9996 sometimes used\n- Choose 2055 for compatibility with legacy tools (LibreNMS, ntop, SolarWinds all support)\n\n**Q13-Q16: \"Network is slow\" root cause analysis**\nData summary:\n- EHR to EHR server: 450 Mbps (legitimate business traffic)\n- YouTube: 320 Mbps (non-work)\n- DNS: 45 Mbps (normal overhead ~50-100 Mbps)\n- Internet total: 100 Mbps available (ISP contract)\n\nQ13: Problem = EHR slow, but YouTube consuming 320 Mbps?\nLikelihood: Impossible. YouTube 320 Mbps would consume 3x available bandwidth.\nActual interpretation: 450 Mbps + 320 Mbps + 45 Mbps = NOT possible on 100 Mbps link.\n\nRe-read scenario: These are probably REQUESTS not flows.\nAlternative: Per-flow basis (individual streams), not aggregate.\n\nActual Q13 answer: EHR appears slow because total Internet bandwidth (450+320+45) = 815 Mbps REQUESTED but only 100 Mbps available = massive congestion and queueing.\n\nQ14: Who causing slowdown?\nAnswer: Everyone. YouTube users consuming 320 Mbps of 100 Mbps ISP contract = 76% to entertainment, only 24% to business.\n\nQ15: Immediate remediation:\n- Apply traffic shaping: Rate-limit YouTube to 20 Mbps (80% of non-business bandwidth allocation)\n- QoS priority: EHR gets priority over entertainment\n- Policing: 100 Mbps total outbound to prevent ISP overages\n\nQ16: Long-term:\n- Upgrade ISP to 300 Mbps (support growth)\n- Implement DLP (Data Loss Prevention) to block non-work applications\n- Implement video streaming detection to redirect to local cache\n\n## ===================================================================\n## SCENARIO 3: CENTRALIZED SYSLOG (SOLUTION)\n## ===================================================================\n\n**Q17: Default syslog severity level**\nAnswer: Informational (7) = default\nCapturesErrors + Warnings + Notices + Informational\nReason: Medical facility needs compliance audit trail, trade-off between verbosity and compliance\n\n**Q18: Problems with logging too much**\n- CPU impact: Device spends cycles on logging, less time routing\n- Disk space: Syslog server fills up (gigabytes per day)\n- Network bandwidth: Syslog traffic consumes link capacity\n- Log analysis impossible: Needle in haystack (millions of log lines)\n- False alerts: Alert system triggered constantly\n\nSolution: Progressive logging\n- Device role based: Core device = Info+, Access device = Error+\n- Time-based: Business hours = Info, off-hours = Error only\n- Event-based: Normal operation = Error, troubleshooting = Debug\n\n**Q19: Logging filter design**\n```\nCore devices (routers, core switches): Log level Informational\nDistribution switches: Log level Warning\nAccess switches: Log level Error\n\nAll devices log: Configuration changes (ccm_mgmt)\nAll devices log: Security violations (AAA)\nCore only: CPU >80%, memory >85%\nDist only: Interface errors >100/day\n```\n\n**Q20: NTP configuration**\nConfiguration:\n```\nntp server 10.1.1.50 prefer         (Primary, preferred)\nntp server 10.1.1.51                (Secondary backup)\nclock timezone PST -8\nclock summer-time PDT recurring\n```\n\n**Q21: Why NTP critical**\nAnswer: Without NTP, device clocks drift 1 second per day\n- 30-day drift: 30 seconds\n- Logs from different devices show events in wrong order\n- Security analysis impossible (which attack came first?)\n- Forensics: \"Device B died, was it after Device A failed?\" - clock skew makes unclear\n- Alert correlation: If events separated by clock error, cannot correlate\n\n**Q22: NTP hierarchy**\n```\n[Internet NTP Pool] (external reference, stratum 0-1)\n    |\n[10.1.1.50 Primary] (stratum 2, serves internal)\n[10.1.1.51 Backup]  (stratum 2, serves internal)\n    |\n[All network devices] (stratum 3, sync to .50 or .51)\n```\n\n**Q23-Q25: Log sequence analysis**\nSequence:\n1. CPU 92%, Memory 4% - Device stressed\n2. Gi0/2 down - Physical/link layer failure\n3. Lost OSPF neighbor 10.1.1.2 - Consequence of Gi0/2 down\n4. BGP route flap - Routing instability due to OSPF loss\n\nRoot cause: Gi0/2 physical failure (overheating, module failure, etc)\nSymptoms: CPU spike (error recovery), memory depletion (error buffers), routing changes\n\n**Q24: Trigger alerts on:**\n- Priority 1 (page on-call): Gi0/2 down (primary link failure)\n- Priority 2 (email): CPU >85% (investigate)\n- Priority 2 (email): Memory <10% (investigate)\n- Priority 3 (log only): BGP route flap (expected consequence)\n- Priority 3 (log only): OSPF neighbor loss (expected after link failure)\n\n**Q25: Cause vs Symptom**\nRoot cause (primary): Gi0/2 interface down\nSymptoms (secondary):\n- OSPF neighbor loss (direct consequence of link down)\n- BGP route flaps (triggered by OSPF change)\n- CPU spike (error recovery, protocol renegotiation)\n- Memory depletion (error buffers, retransmit queues)\n\n## ===================================================================\n## SCENARIO 4: MONITORING DATA TROUBLESHOOTING (SOLUTION)\n## ===================================================================\n\n**Q26: Upgrade timeline**\nData: 95th percentile 78 Mbps, growing 3%/month, ISP contract 100 Mbps\n- Current: 78/100 = 78% utilized\n- Rule: Upgrade when >70% sustained (PASSED already)\n- Projection: 78 Mbps + 3% growth/month = 78 × 1.03^12 ≈ 111 Mbps in 12 months\n- **Recommendation: UPGRADE NOW** (already at 78%, will exceed 100 Mbps in 12 months)\n\n**Q27: Upgrade capacity**\nReasoning:\n- Current peak: 92 Mbps\n- 1-year projection: 111 Mbps\n- 3-year projection: 78 × 1.03^36 ≈ 189 Mbps\n- Growth buffer (2x peak): 189 × 2 = 378 Mbps\n- **Recommendation: 500 Mbps** (accommodate 5+ years growth, allows 2x peak burst)\n\n**Q28: ROI to CFO**\n\"Upgrade from 100 Mbps to 500 Mbps ISP at $2,000/month ($24,000/year) is cost of doing business:\n- Revenue impact: EHR slowdown causes 2-3 hour outage quarterly = $X million\n- Staff productivity: 500 employees × $50/hour × 3 hours = $75,000 loss per outage\n- Compliance: HIPAA requires reliable access to patient records\n- Growth: Supports 5-year growth without emergency upgrade ($50,000 rush fee)\n- Recommendation: Business continuity justifies expense\"\n\n**Q29-Q32: Interface error analysis**\nContext: 247 errors in 30 days\n- 247 errors / 30 days = 8.2 errors/day (average)\n- CRC 198 + frames too long 49 = 247 (accounts for all)\n- Trend: Gradual, not catastrophic\n- Impact: 1-2 retransmissions/hour on 1 Gbps link = negligible\n\nQ29: Significant? No. Errors within normal range (<100/day per best practice, this is 8/day)\nQ30: CRC errors likely cause: \n  - Cable quality (degraded copper, moisture)\n  - Speed/duplex mismatch (1000 Mbps full-duplex switch → 100 Mbps half-duplex device)\n  - NIC driver issues (rare but possible)\n  - Frames too long = Jumbo frame mismatch (switch sends 9000 byte frames, device can only handle 1500)\n\nQ31: Investigation steps:\n  - `show interface Gi0/1` → check speed/duplex (should be 1000 Mbps full-duplex both sides)\n  - Check cabling: Visual inspection, test with tone generator\n  - Check device driver: Is NIC firmware up to date?\n  - Resolve: Disable Jumbo frames if mismatch detected\n\nQ32: \"Monitor and plan upgrade\" situation. Errors not critical yet, but trend worth tracking.\n\n**Q33-Q35: CPU spike investigation**\nData: 65-75% CPU 11am-1pm (finance report generation), baseline 15-25%, weekend 8-12%\n\nQ33: Is this a problem?\nYes, but not immediately critical. Reasons:\n- CPU margin: Running at 65-75% leaves only 25-35% headroom\n- Risk: If additional load (DDoS, misconfiguration), CPU hits 100% causing packet loss\n- Impact: At 100% CPU, router cannot forward traffic = network outage\n\nQ34: Solution?\n- Immediate: Reschedule finance reports to off-hours (2-4am)\n- Medium: Add hardware acceleration (NPU for crypto if available)\n- Long-term: Upgrade router (current model insufficient for load)\n\nQ35: Problem indicator:\n- CPU >85% for >5 minutes (not just spike, sustained load)\n- CPU reaching 100% (packets dropped)\n- CPU spikes correlated with packet loss (show up as user complaints)\n\n**Q36-Q38: Correlation analysis final answer**\nSequence:\n1. **ROOT CAUSE**: DNS server outage (not listed but inferred from \"DNS query timeout\")\n2. **TRIGGER**: 10:32 clients bomb server with DNS retries (normal retry storm)\n3. **SYMPTOM 1**: 10:32 DNS retry traffic = 450 Mbps (consuming Internet link)\n4. **SYMPTOM 2**: 10:35 DNS queries timeout locally (retry exhaustion)\n5. **SYMPTOM 3**: 10:38 Dist-SW-01 CPU spike (processing dropped packets, retransmits)\n6. **SYMPTOM 4**: 10:40 Interface errors increase (packet loss)\n7. **RESOLUTION**: 10:45 DNS restored, traffic normalizes\n\nQ37: Actual root cause:\nDNS server became unreachable (network, not DNS application). Clients in retry loop created traffic storm.\nThis consumed 450 Mbps, leaving no capacity for EHR traffic.\n\nQ38: Monitoring rule to detect BEFORE users notice:\n```\nIF (DNS query traffic > 200 Mbps for >1 minute)\nAND (DNS successful responses < 50%)\nTHEN alert \"DNS server unreachable\" with severity CRITICAL\n\nIF (Any client with >100 DNS queries/minute for >2 minutes)\nTHEN alert \"DNS retry storm detected, investigate DNS availability\"\n\nIF (Interface error rate increases 10x within 5 minutes)\nAND (CPU spike >60% simultaneously)\nTHEN alert \"Packet loss detected, investigate link quality and DNS\"\n```\n\nThese rules would detect the problem at 10:33 (1 minute after traffic spike) instead of 10:30 when users complained.",
  "hints": [
    "SNMPv3 requires authentication AND encryption ('authPriv' or 'priv') for sensitive healthcare data",
    "NetFlow shows actual traffic patterns that SNMP cannot - SNMP only shows interface statistics, not what applications generating traffic",
    "Centralized syslog with NTP is critical for compliance audits and correlating events across multiple devices",
    "Always calculate 95th percentile for capacity planning (ignores brief spikes, shows sustainable load)",
    "DNS retry storms are common cause of bandwidth spikes - a single DNS failure can consume multiple Mbps from retry traffic",
    "Interface errors (CRC, frames too long) indicate physical layer problems - investigate cable quality and speed/duplex mismatch",
    "CPU >85% on network device indicates design limitation - marginal headroom means next traffic spike causes outage",
    "NetFlow on both input and output directions essential - asymmetric traffic impossible to analyze with only one direction",
    "Monitoring data only valuable if actionable - alerts must trigger investigation or automatic remediation",
    "Correlation analysis across SNMP, NetFlow, and Syslog reveals root causes hidden in individual data sources"
  ],
  "testCases": [
    {
      "id": "tc1-snmpv3",
      "description": "SNMPv3 security - Must include authentication and encryption",
      "input": "SNMP configuration",
      "expected": "SNMPv3 with auth sha and priv aes",
      "assert": "assert 'auth sha' in solution and 'priv aes' in solution",
      "isHidden": false
    },
    {
      "id": "tc2-traps",
      "description": "SNMP traps configured for alerts",
      "input": "Alert configuration",
      "expected": "Traps for linkdown, linkup, config, cpu",
      "assert": "assert 'linkdown' in solution and 'linkup' in solution and 'config' in solution",
      "isHidden": false
    },
    {
      "id": "tc3-netflow",
      "description": "NetFlow exporter configuration",
      "input": "NetFlow setup",
      "expected": "Exporter, monitor, and interface configuration",
      "assert": "assert 'flow exporter' in solution and 'flow monitor' in solution and 'ip flow monitor' in solution",
      "isHidden": false
    },
    {
      "id": "tc4-syslog",
      "description": "Centralized syslog with timestamps",
      "input": "Syslog configuration",
      "expected": "Syslog server, severity, timestamps, NTP",
      "assert": "assert 'logging host' in solution and 'service timestamps' in solution and 'ntp server' in solution",
      "isHidden": false
    },
    {
      "id": "tc5-ntp",
      "description": "NTP time synchronization",
      "input": "NTP servers",
      "expected": "Primary and backup NTP servers configured",
      "assert": "assert 'ntp server' in solution",
      "isHidden": false
    },
    {
      "id": "tc6-scenarios",
      "description": "All 4 monitoring scenarios addressed",
      "input": "Scenario coverage",
      "expected": "SNMPv3, NetFlow, Syslog, troubleshooting all covered",
      "assert": "assert 'SCENARIO 1' in solution and 'SCENARIO 2' in solution and 'SCENARIO 3' in solution and 'SCENARIO 4' in solution",
      "isHidden": false
    },
    {
      "id": "tc7-questions",
      "description": "Key monitoring questions answered",
      "input": "Question responses",
      "expected": "38 scenario questions addressed with technical justification",
      "assert": "assert 'Q1' in solution or 'Question 1' in solution.lower() or 'Why use SNMPv3' in solution",
      "isHidden": false
    },
    {
      "id": "tc8-netflow-analysis",
      "description": "NetFlow used to troubleshoot 'network is slow'",
      "input": "Troubleshooting with NetFlow",
      "expected": "NetFlow analysis identifies bandwidth consumers",
      "assert": "assert 'YouTube' in solution or '320 Mbps' in solution or 'EHR' in solution",
      "isHidden": false
    },
    {
      "id": "tc9-rootcause",
      "description": "Root cause identification from correlation",
      "input": "Event correlation",
      "expected": "Identifies DNS as root cause, retry storm as symptom",
      "assert": "assert 'DNS' in solution or 'root cause' in solution.lower() or 'correlation' in solution.lower()",
      "isHidden": false
    },
    {
      "id": "tc10-capacity",
      "description": "Capacity planning from monitoring data",
      "input": "Upgrade recommendation",
      "expected": "95th percentile analysis, growth projection, upgrade timeline",
      "assert": "assert '95th' in solution or 'percentile' in solution.lower() or '3%' in solution or 'upgrade' in solution.lower()",
      "isHidden": false
    },
    {
      "id": "tc11-filesize",
      "description": "Comprehensive monitoring implementation documentation",
      "input": "Document length",
      "expected": "Minimum 12,000 character detailed exercise",
      "assert": "assert len(solution) >= 12000",
      "isHidden": false
    }
  ]
}
